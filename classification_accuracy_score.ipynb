{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de clasificadores con imágenes sintéticas y evaluación del rendimiento con conjunto de test. \n",
    "\n",
    "# Métrica *Classification Accuracy Score*\n",
    "\n",
    "En el presente Notebook, se realiza el entrenamiento de clasificadores con imágenes sintéticas y se evalúa el rendimiento de los mismos con un conjunto de test, siguiendo la filosofía de la métrica *Classification Accuracy Score*, la cual considera que si un modelo generativo genera imágenes de buena calidad, deberían ser buenas para entrenar cualquier clasificador, y que dicho clasificador debería obtener buenas métricas de evaluación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "PIC_CHANNELS = 3\n",
    "PIC_DIMENSION = 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 75\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de las imágenes sintéticas y reales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos reales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las imágenes reales de lesiones de piel, el mismo número para cada clase. De este conjunto total, extraemos una parte para test de los clasificadores que se entrenarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/alzorrcarri/skin_lesion_training_images'\n",
    "real_dataset = []\n",
    "name_files = os.listdir(DATA_DIR)\n",
    "\n",
    "for name in name_files:\n",
    "    if name.endswith('.jpg'):\n",
    "        img = torch.tensor(plt.imread(os.path.join(DATA_DIR, name)), dtype=torch.float32)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        real_dataset.append({'image': img/255.0, 'label': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/alzorrcarri/train_images_benign'\n",
    "\n",
    "name_files = os.listdir(DATA_DIR)\n",
    "\n",
    "for name in name_files:\n",
    "    if name.endswith('.jpg'):\n",
    "        img = torch.tensor(plt.imread(os.path.join(DATA_DIR, name)), dtype=torch.float32)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        real_dataset.append({'image': img/255.0, 'label': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(real_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12602"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = real_dataset[:len(real_dataset)//7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el total de imágenes sintéticas de ambos tipos de lesiones de piel generados con los DDPMs incondicionales entrenados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/alzorrcarri/Library/CloudStorage/Dropbox/tfm/codigo/Métricas de evaluación/malignant/synthetic_images'\n",
    "\n",
    "synthetic_dataset = []\n",
    "name_files = os.listdir(DATA_DIR)\n",
    "\n",
    "for name in name_files:\n",
    "    if name.endswith('.jpg'):\n",
    "        img = torch.tensor(plt.imread(os.path.join(DATA_DIR, name)), dtype=torch.float32)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        synthetic_dataset.append({'image': img/255.0, 'label': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/alzorrcarri/Library/CloudStorage/Dropbox/tfm/codigo/Métricas de evaluación/benign/synthetic_images'\n",
    "\n",
    "name_files = os.listdir(DATA_DIR)\n",
    "\n",
    "for name in name_files:\n",
    "    if name.endswith('.jpg'):\n",
    "        img = torch.tensor(plt.imread(os.path.join(DATA_DIR, name)), dtype=torch.float32)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        synthetic_dataset.append({'image': img/255.0, 'label': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(synthetic_dataset)\n",
    "len(synthetic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor HOG + Clasificador SVM\n",
    "\n",
    "Vamos a usar dos tipos de clasificadores. Por un lado, vamos a tomar un descriptor (en este caso HOG), que extrae ciertas características de las imágenes de entrenamiento, y son con las que vamos a entrenar un clasificador SVM. Usaremos un enfoque de GridSearch para encontrar los mejores parámetros del mismo. El rendimiento del clasificador SVM lo pondremos a prueba con las imágenes de test, sobre las que también se aplicará el descriptor HOG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota: En este caso, no se puede normalizar las imágenes al cargarlas, ya que el descriptor HOG no funciona con imágenes normalizadas. Tampoco se puede establecer las imágenes como tensores de tipo float, sino que deben ser de tipo uint8.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el descriptor HOG de OpenCV y lo aplicamos a las imágenes generadas\n",
    "win_size = (64, 64)\n",
    "block_size = (8, 8)\n",
    "block_stride = (2, 2)\n",
    "cell_size = (4, 4)\n",
    "n_bins_orientacion = 9\n",
    "hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, n_bins_orientacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos el descriptor HOG a las imágenes generadas\n",
    "hog_syn_images = []\n",
    "for data in synthetic_dataset:\n",
    "    img = data['image'].permute(1, 2, 0).numpy()\n",
    "    hog_syn_images.append({'image':hog.compute(img).flatten(), 'label':data['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos el descriptor HOG a las imágenes reales\n",
    "hog_real_images = []\n",
    "for data in real_dataset:\n",
    "    img = data['image'].permute(1, 2, 0).numpy()\n",
    "    hog_real_images.append({'image':hog.compute(img).flatten(), 'label':data['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el conjunto de entrenamiento con el descriptor HOG de cada imagen generada y su etiqueta\n",
    "X_train = []\n",
    "y_train = []\n",
    "for data in hog_syn_images:\n",
    "    X_train.append(data['image'])\n",
    "    y_train.append(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos los mejores parámetros para el clasificador SVM mediante GridSearchCV\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(estimator=svc, param_grid=parameters, scoring='accuracy', cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "best_params = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=10, random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, random_state=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel=best_params['kernel'], C=best_params['C'], random_state=0)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for data in hog_real_images[:len(hog_real_images)//7]:\n",
    "    X_test.append(data['image'])\n",
    "    y_test.append(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4911111111111111\n",
      "Confusion Matrix: \n",
      " [[293 668]\n",
      " [248 591]]\n",
      "Precision:  0.4694201747418586\n",
      "Recall:  0.7044100119189511\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.30      0.39       961\n",
      "           1       0.47      0.70      0.56       839\n",
      "\n",
      "    accuracy                           0.49      1800\n",
      "   macro avg       0.51      0.50      0.48      1800\n",
      "weighted avg       0.51      0.49      0.47      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = svm.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, pred_test))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_labels, pred_test))\n",
    "print(\"Precision: \", precision_score(test_labels, pred_test))\n",
    "print(\"Recall: \", recall_score(test_labels, pred_test))\n",
    "print(\"Classification Report: \\n\", classification_report(test_labels, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales: SkinLesNet\n",
    "\n",
    "Por otro lado, el otro enfoque de clasificación es con un modelo de red neuronal que ya hemos empleado anteriormente para la extracción de carcaterísticas de las imágenes de lesiones de piel, que es la red SkinLesNet. Dicha red se creó originalmente paratareas de clasificación de lesiones de piel y obtuvo muy buenos resultados (https://www.mdpi.com/2613640). Por ello, creemos que es la arquitectura más oportuna para usar en este caso.\n",
    "\n",
    "Con las imágenes sintéticas, creamos un dataset de entrenamiento para la red neuronal. Una vez entrenada, pasamos las imágenes de test por la red neuronal y comparamos las etiquetas predichas con las reales, y calculamos distintas métricas de rendimiento del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer enfoque: entrenar solo con imágenes sintéticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el clasificador solo con imágenes sintéticas y evaluamos su rendimiento con las imágenes de test, que son imágenes reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(synthetic_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLesNet(nn.Module):\n",
    "    def __init__(self, IMAGE_SIZE):\n",
    "        super(SkinLesNet, self).__init__()\n",
    "        # 1st Convolutional Input Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 2nd Convolutional Input Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 3rd Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 4th Convolutional Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * (IMAGE_SIZE // 16) * (IMAGE_SIZE // 16), 64)  # Adjusting for downsampling\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tfm/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "skinlesnet = SkinLesNet(IMAGE_SIZE).to(DEVICE)\n",
    "optimizer = optim.Adam(skinlesnet.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr_sch = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-15, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == target).sum().item()\n",
    "    return correct / target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 32/32 [00:01<00:00, 19.11it/s, accuracy=0.632, loss=0.639]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 92.19it/s, accuracy=0.756, loss=0.54] \n",
      "Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 95.38it/s, accuracy=0.842, loss=0.469]\n",
      "Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 94.88it/s, accuracy=0.831, loss=0.481]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 85.26it/s, accuracy=0.84, loss=0.472] \n",
      "Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 95.76it/s, accuracy=0.879, loss=0.439]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 93.66it/s, accuracy=0.87, loss=0.44]  \n",
      "Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 96.33it/s, accuracy=0.892, loss=0.424]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 95.33it/s, accuracy=0.893, loss=0.42] \n",
      "Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 94.54it/s, accuracy=0.89, loss=0.423] \n",
      "Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 85.71it/s, accuracy=0.899, loss=0.415]\n",
      "Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 86.83it/s, accuracy=0.897, loss=0.413]\n",
      "Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 96.30it/s, accuracy=0.901, loss=0.407]\n",
      "Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 95.40it/s, accuracy=0.905, loss=0.405]\n",
      "Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 95.07it/s, accuracy=0.902, loss=0.408]\n",
      "Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 91.49it/s, accuracy=0.894, loss=0.413]\n",
      "Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 93.81it/s, accuracy=0.916, loss=0.397]\n",
      "Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 85.10it/s, accuracy=0.921, loss=0.394]\n",
      "Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 94.58it/s, accuracy=0.911, loss=0.398]\n",
      "Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 95.31it/s, accuracy=0.922, loss=0.391]\n",
      "Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 90.01it/s, accuracy=0.909, loss=0.393]\n",
      "Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 91.30it/s, accuracy=0.924, loss=0.39] \n",
      "Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 94.27it/s, accuracy=0.922, loss=0.389]\n",
      "Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 78.21it/s, accuracy=0.931, loss=0.383]\n",
      "Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 91.26it/s, accuracy=0.924, loss=0.388]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 94.39it/s, accuracy=0.932, loss=0.381]\n",
      "Epoch 26: 100%|██████████| 32/32 [00:00<00:00, 94.89it/s, accuracy=0.934, loss=0.381]\n",
      "Epoch 27: 100%|██████████| 32/32 [00:00<00:00, 94.49it/s, accuracy=0.932, loss=0.378]\n",
      "Epoch 28: 100%|██████████| 32/32 [00:00<00:00, 79.76it/s, accuracy=0.939, loss=0.374]\n",
      "Epoch 29: 100%|██████████| 32/32 [00:00<00:00, 95.08it/s, accuracy=0.935, loss=0.377]\n",
      "Epoch 30: 100%|██████████| 32/32 [00:00<00:00, 91.07it/s, accuracy=0.94, loss=0.376] \n",
      "Epoch 31: 100%|██████████| 32/32 [00:00<00:00, 91.60it/s, accuracy=0.931, loss=0.376]\n",
      "Epoch 32: 100%|██████████| 32/32 [00:00<00:00, 93.41it/s, accuracy=0.941, loss=0.373]\n",
      "Epoch 33: 100%|██████████| 32/32 [00:00<00:00, 84.28it/s, accuracy=0.947, loss=0.371]\n",
      "Epoch 34: 100%|██████████| 32/32 [00:00<00:00, 91.01it/s, accuracy=0.939, loss=0.37] \n",
      "Epoch 35: 100%|██████████| 32/32 [00:00<00:00, 92.53it/s, accuracy=0.935, loss=0.374]\n",
      "Epoch 36: 100%|██████████| 32/32 [00:00<00:00, 91.85it/s, accuracy=0.944, loss=0.37] \n",
      "Epoch 37: 100%|██████████| 32/32 [00:00<00:00, 92.57it/s, accuracy=0.947, loss=0.366]\n",
      "Epoch 38: 100%|██████████| 32/32 [00:00<00:00, 81.80it/s, accuracy=0.941, loss=0.37] \n",
      "Epoch 39: 100%|██████████| 32/32 [00:00<00:00, 89.27it/s, accuracy=0.95, loss=0.366] \n",
      "Epoch 40: 100%|██████████| 32/32 [00:00<00:00, 87.37it/s, accuracy=0.939, loss=0.37] \n",
      "Epoch 41: 100%|██████████| 32/32 [00:00<00:00, 91.83it/s, accuracy=0.949, loss=0.364]\n",
      "Epoch 42: 100%|██████████| 32/32 [00:00<00:00, 91.46it/s, accuracy=0.96, loss=0.36]  \n",
      "Epoch 43: 100%|██████████| 32/32 [00:00<00:00, 87.44it/s, accuracy=0.946, loss=0.367]\n",
      "Epoch 44: 100%|██████████| 32/32 [00:00<00:00, 87.72it/s, accuracy=0.952, loss=0.362]\n",
      "Epoch 45: 100%|██████████| 32/32 [00:00<00:00, 94.83it/s, accuracy=0.948, loss=0.367]\n",
      "Epoch 46: 100%|██████████| 32/32 [00:00<00:00, 95.66it/s, accuracy=0.951, loss=0.365]\n",
      "Epoch 47: 100%|██████████| 32/32 [00:00<00:00, 90.12it/s, accuracy=0.954, loss=0.361]\n",
      "Epoch 48: 100%|██████████| 32/32 [00:00<00:00, 85.06it/s, accuracy=0.951, loss=0.363]\n",
      "Epoch 49: 100%|██████████| 32/32 [00:00<00:00, 93.34it/s, accuracy=0.956, loss=0.361]\n",
      "Epoch 50: 100%|██████████| 32/32 [00:00<00:00, 90.59it/s, accuracy=0.954, loss=0.362]\n",
      "Epoch 51: 100%|██████████| 32/32 [00:00<00:00, 78.55it/s, accuracy=0.953, loss=0.362]\n",
      "Epoch 52: 100%|██████████| 32/32 [00:00<00:00, 88.08it/s, accuracy=0.952, loss=0.362]\n",
      "Epoch 53: 100%|██████████| 32/32 [00:00<00:00, 92.51it/s, accuracy=0.958, loss=0.361]\n",
      "Epoch 54: 100%|██████████| 32/32 [00:00<00:00, 95.47it/s, accuracy=0.955, loss=0.362]\n",
      "Epoch 55: 100%|██████████| 32/32 [00:00<00:00, 91.78it/s, accuracy=0.953, loss=0.361]\n",
      "Epoch 56: 100%|██████████| 32/32 [00:00<00:00, 85.32it/s, accuracy=0.95, loss=0.362] \n",
      "Epoch 57: 100%|██████████| 32/32 [00:00<00:00, 90.68it/s, accuracy=0.951, loss=0.364]\n",
      "Epoch 58: 100%|██████████| 32/32 [00:00<00:00, 93.88it/s, accuracy=0.95, loss=0.363] \n",
      "Epoch 59: 100%|██████████| 32/32 [00:00<00:00, 94.91it/s, accuracy=0.948, loss=0.366]\n",
      "Epoch 60: 100%|██████████| 32/32 [00:00<00:00, 81.76it/s, accuracy=0.954, loss=0.363]\n",
      "Epoch 61: 100%|██████████| 32/32 [00:00<00:00, 93.63it/s, accuracy=0.952, loss=0.363]\n",
      "Epoch 62: 100%|██████████| 32/32 [00:00<00:00, 91.84it/s, accuracy=0.959, loss=0.36] \n",
      "Epoch 63: 100%|██████████| 32/32 [00:00<00:00, 94.41it/s, accuracy=0.956, loss=0.359]\n",
      "Epoch 64: 100%|██████████| 32/32 [00:00<00:00, 84.47it/s, accuracy=0.952, loss=0.366]\n",
      "Epoch 65: 100%|██████████| 32/32 [00:00<00:00, 85.70it/s, accuracy=0.948, loss=0.366]\n",
      "Epoch 66: 100%|██████████| 32/32 [00:00<00:00, 90.85it/s, accuracy=0.956, loss=0.36] \n",
      "Epoch 67: 100%|██████████| 32/32 [00:00<00:00, 92.20it/s, accuracy=0.948, loss=0.364]\n",
      "Epoch 68: 100%|██████████| 32/32 [00:00<00:00, 86.67it/s, accuracy=0.952, loss=0.362]\n",
      "Epoch 69: 100%|██████████| 32/32 [00:00<00:00, 91.31it/s, accuracy=0.957, loss=0.36] \n",
      "Epoch 70: 100%|██████████| 32/32 [00:00<00:00, 93.54it/s, accuracy=0.955, loss=0.36] \n",
      "Epoch 71: 100%|██████████| 32/32 [00:00<00:00, 93.16it/s, accuracy=0.953, loss=0.361]\n",
      "Epoch 72: 100%|██████████| 32/32 [00:00<00:00, 88.74it/s, accuracy=0.951, loss=0.364]\n",
      "Epoch 73: 100%|██████████| 32/32 [00:00<00:00, 96.45it/s, accuracy=0.957, loss=0.357]\n",
      "Epoch 74: 100%|██████████| 32/32 [00:00<00:00, 92.16it/s, accuracy=0.955, loss=0.36] \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    skinlesnet.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images_batch = batch['image'].to(DEVICE)\n",
    "        labels_batch = batch['label'].to(DEVICE).long()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        output = skinlesnet(images_batch)\n",
    "        loss_value = loss(output, labels_batch)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.item()\n",
    "        epoch_accuracy += calculate_accuracy(output, labels_batch)\n",
    "        progress_bar.set_postfix(loss=epoch_loss / (step + 1), accuracy=epoch_accuracy / (step + 1))\n",
    "    lr_sch.step(epoch_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de test\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i in range(len(test_dataset)):\n",
    "    test_images.append(test_dataset[i]['image'])\n",
    "    test_labels.append(test_dataset[i]['label'])\n",
    "\n",
    "test_images = torch.stack(test_images)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7122222222222222\n",
      "Confusion Matrix: \n",
      " [[660 301]\n",
      " [217 622]]\n",
      "Precision:  0.6738894907908992\n",
      "Recall:  0.7413587604290822\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.69      0.72       961\n",
      "           1       0.67      0.74      0.71       839\n",
      "\n",
      "    accuracy                           0.71      1800\n",
      "   macro avg       0.71      0.71      0.71      1800\n",
      "weighted avg       0.72      0.71      0.71      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo en el conjunto de test\n",
    "skinlesnet.eval()\n",
    "pred_test = skinlesnet(test_images.to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, pred_test))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_labels, pred_test))\n",
    "print(\"Precision: \", precision_score(test_labels, pred_test))\n",
    "print(\"Recall: \", recall_score(test_labels, pred_test))\n",
    "print(\"Classification Report: \\n\", classification_report(test_labels, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo enfoque: entrenar con imágenes reales y sintéticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos el clasificador con las imágenes reales y sintéticas, y evaluamos su rendimiento con las imágenes de test, que son únicamente reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = real_dataset[len(real_dataset)//7:] + synthetic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "skinlesnet = SkinLesNet(IMAGE_SIZE).to(DEVICE)\n",
    "optimizer = optim.Adam(skinlesnet.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr_sch = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-15, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 369/369 [00:04<00:00, 86.60it/s, accuracy=0.757, loss=0.54] \n",
      "Epoch 1: 100%|██████████| 369/369 [00:04<00:00, 88.35it/s, accuracy=0.794, loss=0.508]\n",
      "Epoch 2: 100%|██████████| 369/369 [00:04<00:00, 91.82it/s, accuracy=0.802, loss=0.499]\n",
      "Epoch 3: 100%|██████████| 369/369 [00:04<00:00, 91.41it/s, accuracy=0.807, loss=0.493]\n",
      "Epoch 4: 100%|██████████| 369/369 [00:04<00:00, 91.95it/s, accuracy=0.813, loss=0.486]\n",
      "Epoch 5: 100%|██████████| 369/369 [00:03<00:00, 92.54it/s, accuracy=0.822, loss=0.481]\n",
      "Epoch 6: 100%|██████████| 369/369 [00:03<00:00, 93.48it/s, accuracy=0.818, loss=0.481]\n",
      "Epoch 7: 100%|██████████| 369/369 [00:03<00:00, 92.40it/s, accuracy=0.824, loss=0.476]\n",
      "Epoch 8: 100%|██████████| 369/369 [00:03<00:00, 93.01it/s, accuracy=0.825, loss=0.478]\n",
      "Epoch 9: 100%|██████████| 369/369 [00:03<00:00, 93.22it/s, accuracy=0.827, loss=0.475]\n",
      "Epoch 10: 100%|██████████| 369/369 [00:04<00:00, 90.45it/s, accuracy=0.83, loss=0.472] \n",
      "Epoch 11: 100%|██████████| 369/369 [00:03<00:00, 93.19it/s, accuracy=0.834, loss=0.469]\n",
      "Epoch 12: 100%|██████████| 369/369 [00:04<00:00, 91.39it/s, accuracy=0.837, loss=0.467]\n",
      "Epoch 13: 100%|██████████| 369/369 [00:03<00:00, 93.61it/s, accuracy=0.839, loss=0.465]\n",
      "Epoch 14: 100%|██████████| 369/369 [00:03<00:00, 92.59it/s, accuracy=0.841, loss=0.463]\n",
      "Epoch 15: 100%|██████████| 369/369 [00:04<00:00, 91.67it/s, accuracy=0.842, loss=0.461]\n",
      "Epoch 16: 100%|██████████| 369/369 [00:04<00:00, 91.69it/s, accuracy=0.847, loss=0.456]\n",
      "Epoch 17: 100%|██████████| 369/369 [00:04<00:00, 92.00it/s, accuracy=0.846, loss=0.459]\n",
      "Epoch 18: 100%|██████████| 369/369 [00:04<00:00, 91.40it/s, accuracy=0.85, loss=0.456] \n",
      "Epoch 19: 100%|██████████| 369/369 [00:04<00:00, 91.54it/s, accuracy=0.855, loss=0.452]\n",
      "Epoch 20: 100%|██████████| 369/369 [00:04<00:00, 91.04it/s, accuracy=0.854, loss=0.453]\n",
      "Epoch 21: 100%|██████████| 369/369 [00:04<00:00, 91.75it/s, accuracy=0.859, loss=0.448]\n",
      "Epoch 22: 100%|██████████| 369/369 [00:04<00:00, 90.30it/s, accuracy=0.863, loss=0.445]\n",
      "Epoch 23: 100%|██████████| 369/369 [00:04<00:00, 91.29it/s, accuracy=0.863, loss=0.444]\n",
      "Epoch 24: 100%|██████████| 369/369 [00:04<00:00, 90.28it/s, accuracy=0.861, loss=0.448]\n",
      "Epoch 25: 100%|██████████| 369/369 [00:04<00:00, 91.39it/s, accuracy=0.867, loss=0.439]\n",
      "Epoch 26: 100%|██████████| 369/369 [00:04<00:00, 90.34it/s, accuracy=0.868, loss=0.439]\n",
      "Epoch 27: 100%|██████████| 369/369 [00:04<00:00, 89.09it/s, accuracy=0.871, loss=0.436]\n",
      "Epoch 28: 100%|██████████| 369/369 [00:04<00:00, 91.34it/s, accuracy=0.875, loss=0.432]\n",
      "Epoch 29: 100%|██████████| 369/369 [00:04<00:00, 91.67it/s, accuracy=0.877, loss=0.431]\n",
      "Epoch 30: 100%|██████████| 369/369 [00:03<00:00, 92.87it/s, accuracy=0.879, loss=0.43] \n",
      "Epoch 31: 100%|██████████| 369/369 [00:04<00:00, 91.46it/s, accuracy=0.883, loss=0.427]\n",
      "Epoch 32: 100%|██████████| 369/369 [00:04<00:00, 90.93it/s, accuracy=0.888, loss=0.423]\n",
      "Epoch 33: 100%|██████████| 369/369 [00:03<00:00, 93.17it/s, accuracy=0.884, loss=0.425]\n",
      "Epoch 34: 100%|██████████| 369/369 [00:04<00:00, 92.18it/s, accuracy=0.884, loss=0.425]\n",
      "Epoch 35: 100%|██████████| 369/369 [00:04<00:00, 91.33it/s, accuracy=0.898, loss=0.412]\n",
      "Epoch 36: 100%|██████████| 369/369 [00:04<00:00, 92.19it/s, accuracy=0.908, loss=0.404]\n",
      "Epoch 37: 100%|██████████| 369/369 [00:04<00:00, 91.18it/s, accuracy=0.911, loss=0.4]  \n",
      "Epoch 38: 100%|██████████| 369/369 [00:04<00:00, 91.88it/s, accuracy=0.91, loss=0.4]   \n",
      "Epoch 39: 100%|██████████| 369/369 [00:04<00:00, 91.90it/s, accuracy=0.909, loss=0.402]\n",
      "Epoch 40: 100%|██████████| 369/369 [00:03<00:00, 92.30it/s, accuracy=0.914, loss=0.397]\n",
      "Epoch 41: 100%|██████████| 369/369 [00:04<00:00, 91.41it/s, accuracy=0.917, loss=0.394]\n",
      "Epoch 42: 100%|██████████| 369/369 [00:04<00:00, 91.23it/s, accuracy=0.916, loss=0.395]\n",
      "Epoch 43: 100%|██████████| 369/369 [00:03<00:00, 92.61it/s, accuracy=0.917, loss=0.393]\n",
      "Epoch 44: 100%|██████████| 369/369 [00:04<00:00, 91.55it/s, accuracy=0.921, loss=0.39] \n",
      "Epoch 45: 100%|██████████| 369/369 [00:04<00:00, 91.77it/s, accuracy=0.92, loss=0.391] \n",
      "Epoch 46: 100%|██████████| 369/369 [00:04<00:00, 91.49it/s, accuracy=0.92, loss=0.39]  \n",
      "Epoch 47: 100%|██████████| 369/369 [00:04<00:00, 91.09it/s, accuracy=0.924, loss=0.388]\n",
      "Epoch 48: 100%|██████████| 369/369 [00:04<00:00, 90.70it/s, accuracy=0.928, loss=0.384]\n",
      "Epoch 49: 100%|██████████| 369/369 [00:04<00:00, 91.30it/s, accuracy=0.925, loss=0.387]\n",
      "Epoch 50: 100%|██████████| 369/369 [00:04<00:00, 90.88it/s, accuracy=0.928, loss=0.384]\n",
      "Epoch 51: 100%|██████████| 369/369 [00:04<00:00, 90.79it/s, accuracy=0.93, loss=0.382] \n",
      "Epoch 52: 100%|██████████| 369/369 [00:04<00:00, 91.50it/s, accuracy=0.931, loss=0.381]\n",
      "Epoch 53: 100%|██████████| 369/369 [00:04<00:00, 91.22it/s, accuracy=0.932, loss=0.38] \n",
      "Epoch 54: 100%|██████████| 369/369 [00:04<00:00, 90.55it/s, accuracy=0.933, loss=0.379]\n",
      "Epoch 55: 100%|██████████| 369/369 [00:04<00:00, 90.83it/s, accuracy=0.932, loss=0.379]\n",
      "Epoch 56: 100%|██████████| 369/369 [00:04<00:00, 91.85it/s, accuracy=0.937, loss=0.376]\n",
      "Epoch 57: 100%|██████████| 369/369 [00:04<00:00, 90.71it/s, accuracy=0.936, loss=0.377]\n",
      "Epoch 58: 100%|██████████| 369/369 [00:04<00:00, 90.59it/s, accuracy=0.936, loss=0.377]\n",
      "Epoch 59: 100%|██████████| 369/369 [00:04<00:00, 91.59it/s, accuracy=0.945, loss=0.368]\n",
      "Epoch 60: 100%|██████████| 369/369 [00:04<00:00, 91.22it/s, accuracy=0.944, loss=0.369]\n",
      "Epoch 61: 100%|██████████| 369/369 [00:04<00:00, 90.39it/s, accuracy=0.948, loss=0.365]\n",
      "Epoch 62: 100%|██████████| 369/369 [00:04<00:00, 91.32it/s, accuracy=0.945, loss=0.367]\n",
      "Epoch 63: 100%|██████████| 369/369 [00:04<00:00, 90.21it/s, accuracy=0.947, loss=0.366]\n",
      "Epoch 64: 100%|██████████| 369/369 [00:04<00:00, 91.78it/s, accuracy=0.951, loss=0.362]\n",
      "Epoch 65: 100%|██████████| 369/369 [00:04<00:00, 91.72it/s, accuracy=0.951, loss=0.362]\n",
      "Epoch 66: 100%|██████████| 369/369 [00:04<00:00, 90.50it/s, accuracy=0.955, loss=0.359]\n",
      "Epoch 67: 100%|██████████| 369/369 [00:04<00:00, 90.68it/s, accuracy=0.953, loss=0.361]\n",
      "Epoch 68: 100%|██████████| 369/369 [00:04<00:00, 90.04it/s, accuracy=0.951, loss=0.362]\n",
      "Epoch 69: 100%|██████████| 369/369 [00:04<00:00, 91.68it/s, accuracy=0.954, loss=0.359]\n",
      "Epoch 70: 100%|██████████| 369/369 [00:04<00:00, 90.75it/s, accuracy=0.955, loss=0.358]\n",
      "Epoch 71: 100%|██████████| 369/369 [00:04<00:00, 89.88it/s, accuracy=0.954, loss=0.358]\n",
      "Epoch 72: 100%|██████████| 369/369 [00:04<00:00, 90.52it/s, accuracy=0.955, loss=0.358]\n",
      "Epoch 73: 100%|██████████| 369/369 [00:04<00:00, 92.16it/s, accuracy=0.957, loss=0.357]\n",
      "Epoch 74: 100%|██████████| 369/369 [00:04<00:00, 89.44it/s, accuracy=0.956, loss=0.357]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    skinlesnet.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images_batch = batch['image'].to(DEVICE)\n",
    "        labels_batch = batch['label'].to(DEVICE).long()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        output = skinlesnet(images_batch)\n",
    "        loss_value = loss(output, labels_batch)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.item()\n",
    "        epoch_accuracy += calculate_accuracy(output, labels_batch)\n",
    "        progress_bar.set_postfix(loss=epoch_loss / (step + 1), accuracy=epoch_accuracy / (step + 1))\n",
    "    lr_sch.step(epoch_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8533333333333334\n",
      "Confusion Matrix: \n",
      " [[838 123]\n",
      " [141 698]]\n",
      "Precision:  0.8501827040194885\n",
      "Recall:  0.831942789034565\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       961\n",
      "           1       0.85      0.83      0.84       839\n",
      "\n",
      "    accuracy                           0.85      1800\n",
      "   macro avg       0.85      0.85      0.85      1800\n",
      "weighted avg       0.85      0.85      0.85      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo en el conjunto de test\n",
    "skinlesnet.eval()\n",
    "pred_test = skinlesnet(test_images.to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, pred_test))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_labels, pred_test))\n",
    "print(\"Precision: \", precision_score(test_labels, pred_test))\n",
    "print(\"Recall: \", recall_score(test_labels, pred_test))\n",
    "print(\"Classification Report: \\n\", classification_report(test_labels, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red solo con el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenamos la red únicamente con las imágenes reales y evaluamos su rendimiento con las imágenes de test, para así tener un marco de referencia para comparar con los resultados anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(real_dataset[len(real_dataset)//7:], batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tfm/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "skinlesnet = SkinLesNet(IMAGE_SIZE).to(DEVICE)\n",
    "optimizer = optim.Adam(skinlesnet.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr_sch = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-15, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 338/338 [00:04<00:00, 80.58it/s, accuracy=0.772, loss=0.525]\n",
      "Epoch 1: 100%|██████████| 338/338 [00:03<00:00, 87.58it/s, accuracy=0.811, loss=0.489]\n",
      "Epoch 2: 100%|██████████| 338/338 [00:03<00:00, 92.87it/s, accuracy=0.813, loss=0.488]\n",
      "Epoch 3: 100%|██████████| 338/338 [00:03<00:00, 91.91it/s, accuracy=0.819, loss=0.481]\n",
      "Epoch 4: 100%|██████████| 338/338 [00:03<00:00, 91.20it/s, accuracy=0.819, loss=0.482]\n",
      "Epoch 5: 100%|██████████| 338/338 [00:03<00:00, 93.35it/s, accuracy=0.815, loss=0.484]\n",
      "Epoch 6: 100%|██████████| 338/338 [00:03<00:00, 92.07it/s, accuracy=0.829, loss=0.472]\n",
      "Epoch 7: 100%|██████████| 338/338 [00:03<00:00, 91.68it/s, accuracy=0.832, loss=0.471]\n",
      "Epoch 8: 100%|██████████| 338/338 [00:03<00:00, 92.04it/s, accuracy=0.831, loss=0.469]\n",
      "Epoch 9: 100%|██████████| 338/338 [00:03<00:00, 92.91it/s, accuracy=0.833, loss=0.47] \n",
      "Epoch 10: 100%|██████████| 338/338 [00:03<00:00, 91.40it/s, accuracy=0.838, loss=0.466]\n",
      "Epoch 11: 100%|██████████| 338/338 [00:03<00:00, 92.60it/s, accuracy=0.835, loss=0.467]\n",
      "Epoch 12: 100%|██████████| 338/338 [00:03<00:00, 91.33it/s, accuracy=0.837, loss=0.465]\n",
      "Epoch 13: 100%|██████████| 338/338 [00:03<00:00, 92.36it/s, accuracy=0.845, loss=0.461]\n",
      "Epoch 14: 100%|██████████| 338/338 [00:03<00:00, 90.59it/s, accuracy=0.846, loss=0.46] \n",
      "Epoch 15: 100%|██████████| 338/338 [00:03<00:00, 91.85it/s, accuracy=0.847, loss=0.459]\n",
      "Epoch 16: 100%|██████████| 338/338 [00:03<00:00, 91.20it/s, accuracy=0.851, loss=0.454]\n",
      "Epoch 17: 100%|██████████| 338/338 [00:03<00:00, 92.43it/s, accuracy=0.85, loss=0.456] \n",
      "Epoch 18: 100%|██████████| 338/338 [00:03<00:00, 89.77it/s, accuracy=0.85, loss=0.454] \n",
      "Epoch 19: 100%|██████████| 338/338 [00:03<00:00, 90.34it/s, accuracy=0.855, loss=0.451]\n",
      "Epoch 20: 100%|██████████| 338/338 [00:03<00:00, 92.02it/s, accuracy=0.857, loss=0.45] \n",
      "Epoch 21: 100%|██████████| 338/338 [00:03<00:00, 90.15it/s, accuracy=0.855, loss=0.452]\n",
      "Epoch 22: 100%|██████████| 338/338 [00:03<00:00, 91.99it/s, accuracy=0.86, loss=0.446] \n",
      "Epoch 23: 100%|██████████| 338/338 [00:03<00:00, 93.60it/s, accuracy=0.864, loss=0.444]\n",
      "Epoch 24: 100%|██████████| 338/338 [00:03<00:00, 90.71it/s, accuracy=0.868, loss=0.439]\n",
      "Epoch 25: 100%|██████████| 338/338 [00:03<00:00, 91.78it/s, accuracy=0.868, loss=0.439]\n",
      "Epoch 26: 100%|██████████| 338/338 [00:03<00:00, 92.47it/s, accuracy=0.871, loss=0.438]\n",
      "Epoch 27: 100%|██████████| 338/338 [00:03<00:00, 89.84it/s, accuracy=0.875, loss=0.436]\n",
      "Epoch 28: 100%|██████████| 338/338 [00:03<00:00, 90.97it/s, accuracy=0.875, loss=0.434]\n",
      "Epoch 29: 100%|██████████| 338/338 [00:03<00:00, 92.97it/s, accuracy=0.881, loss=0.428]\n",
      "Epoch 30: 100%|██████████| 338/338 [00:03<00:00, 91.50it/s, accuracy=0.881, loss=0.428]\n",
      "Epoch 31: 100%|██████████| 338/338 [00:03<00:00, 90.49it/s, accuracy=0.885, loss=0.425]\n",
      "Epoch 32: 100%|██████████| 338/338 [00:03<00:00, 90.35it/s, accuracy=0.888, loss=0.423]\n",
      "Epoch 33: 100%|██████████| 338/338 [00:03<00:00, 91.61it/s, accuracy=0.886, loss=0.423]\n",
      "Epoch 34: 100%|██████████| 338/338 [00:03<00:00, 90.40it/s, accuracy=0.894, loss=0.416]\n",
      "Epoch 35: 100%|██████████| 338/338 [00:03<00:00, 91.64it/s, accuracy=0.897, loss=0.414]\n",
      "Epoch 36: 100%|██████████| 338/338 [00:03<00:00, 91.76it/s, accuracy=0.899, loss=0.413]\n",
      "Epoch 37: 100%|██████████| 338/338 [00:03<00:00, 91.36it/s, accuracy=0.903, loss=0.41] \n",
      "Epoch 38: 100%|██████████| 338/338 [00:03<00:00, 92.82it/s, accuracy=0.901, loss=0.409]\n",
      "Epoch 39: 100%|██████████| 338/338 [00:03<00:00, 93.40it/s, accuracy=0.906, loss=0.404]\n",
      "Epoch 40: 100%|██████████| 338/338 [00:03<00:00, 92.33it/s, accuracy=0.903, loss=0.408]\n",
      "Epoch 41: 100%|██████████| 338/338 [00:03<00:00, 87.61it/s, accuracy=0.907, loss=0.404]\n",
      "Epoch 42: 100%|██████████| 338/338 [00:03<00:00, 88.65it/s, accuracy=0.917, loss=0.395]\n",
      "Epoch 43: 100%|██████████| 338/338 [00:03<00:00, 90.23it/s, accuracy=0.922, loss=0.39] \n",
      "Epoch 44: 100%|██████████| 338/338 [00:03<00:00, 89.10it/s, accuracy=0.923, loss=0.389]\n",
      "Epoch 45: 100%|██████████| 338/338 [00:03<00:00, 90.70it/s, accuracy=0.925, loss=0.387]\n",
      "Epoch 46: 100%|██████████| 338/338 [00:03<00:00, 90.50it/s, accuracy=0.93, loss=0.384] \n",
      "Epoch 47: 100%|██████████| 338/338 [00:03<00:00, 89.00it/s, accuracy=0.932, loss=0.382]\n",
      "Epoch 48: 100%|██████████| 338/338 [00:03<00:00, 90.07it/s, accuracy=0.929, loss=0.383]\n",
      "Epoch 49: 100%|██████████| 338/338 [00:03<00:00, 89.43it/s, accuracy=0.932, loss=0.38] \n",
      "Epoch 50: 100%|██████████| 338/338 [00:03<00:00, 89.37it/s, accuracy=0.928, loss=0.383]\n",
      "Epoch 51: 100%|██████████| 338/338 [00:03<00:00, 89.09it/s, accuracy=0.936, loss=0.377]\n",
      "Epoch 52: 100%|██████████| 338/338 [00:03<00:00, 89.37it/s, accuracy=0.937, loss=0.377]\n",
      "Epoch 53: 100%|██████████| 338/338 [00:03<00:00, 89.69it/s, accuracy=0.933, loss=0.379]\n",
      "Epoch 54: 100%|██████████| 338/338 [00:03<00:00, 89.25it/s, accuracy=0.938, loss=0.375]\n",
      "Epoch 55: 100%|██████████| 338/338 [00:03<00:00, 90.09it/s, accuracy=0.938, loss=0.376]\n",
      "Epoch 56: 100%|██████████| 338/338 [00:03<00:00, 89.83it/s, accuracy=0.941, loss=0.372]\n",
      "Epoch 57: 100%|██████████| 338/338 [00:03<00:00, 91.37it/s, accuracy=0.943, loss=0.37] \n",
      "Epoch 58: 100%|██████████| 338/338 [00:03<00:00, 88.66it/s, accuracy=0.942, loss=0.371]\n",
      "Epoch 59: 100%|██████████| 338/338 [00:03<00:00, 90.25it/s, accuracy=0.942, loss=0.371]\n",
      "Epoch 60: 100%|██████████| 338/338 [00:03<00:00, 90.04it/s, accuracy=0.947, loss=0.366]\n",
      "Epoch 61: 100%|██████████| 338/338 [00:03<00:00, 88.24it/s, accuracy=0.949, loss=0.364]\n",
      "Epoch 62: 100%|██████████| 338/338 [00:03<00:00, 89.47it/s, accuracy=0.953, loss=0.36] \n",
      "Epoch 63: 100%|██████████| 338/338 [00:03<00:00, 88.76it/s, accuracy=0.95, loss=0.364] \n",
      "Epoch 64: 100%|██████████| 338/338 [00:03<00:00, 89.94it/s, accuracy=0.952, loss=0.362]\n",
      "Epoch 65: 100%|██████████| 338/338 [00:03<00:00, 90.14it/s, accuracy=0.955, loss=0.359]\n",
      "Epoch 66: 100%|██████████| 338/338 [00:03<00:00, 89.74it/s, accuracy=0.956, loss=0.359]\n",
      "Epoch 67: 100%|██████████| 338/338 [00:03<00:00, 88.99it/s, accuracy=0.956, loss=0.358]\n",
      "Epoch 68: 100%|██████████| 338/338 [00:03<00:00, 88.22it/s, accuracy=0.956, loss=0.358]\n",
      "Epoch 69: 100%|██████████| 338/338 [00:03<00:00, 87.37it/s, accuracy=0.957, loss=0.357]\n",
      "Epoch 70: 100%|██████████| 338/338 [00:03<00:00, 88.58it/s, accuracy=0.956, loss=0.357]\n",
      "Epoch 71: 100%|██████████| 338/338 [00:03<00:00, 89.06it/s, accuracy=0.958, loss=0.356]\n",
      "Epoch 72: 100%|██████████| 338/338 [00:03<00:00, 86.66it/s, accuracy=0.959, loss=0.356]\n",
      "Epoch 73: 100%|██████████| 338/338 [00:04<00:00, 84.17it/s, accuracy=0.958, loss=0.356]\n",
      "Epoch 74: 100%|██████████| 338/338 [00:03<00:00, 85.71it/s, accuracy=0.959, loss=0.355]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    skinlesnet.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images_batch = batch['image'].to(DEVICE)\n",
    "        labels_batch = batch['label'].to(DEVICE).long()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        output = skinlesnet(images_batch)\n",
    "        loss_value = loss(output, labels_batch)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.item()\n",
    "        epoch_accuracy += calculate_accuracy(output, labels_batch)\n",
    "        progress_bar.set_postfix(loss=epoch_loss / (step + 1), accuracy=epoch_accuracy / (step + 1))\n",
    "    lr_sch.step(epoch_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.86\n",
      "Confusion Matrix: \n",
      " [[863  98]\n",
      " [154 685]]\n",
      "Precision:  0.8748403575989783\n",
      "Recall:  0.8164481525625745\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87       961\n",
      "           1       0.87      0.82      0.84       839\n",
      "\n",
      "    accuracy                           0.86      1800\n",
      "   macro avg       0.86      0.86      0.86      1800\n",
      "weighted avg       0.86      0.86      0.86      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo en el conjunto de test\n",
    "skinlesnet.eval()\n",
    "pred_test = skinlesnet(test_images.to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, pred_test))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_labels, pred_test))\n",
    "print(\"Precision: \", precision_score(test_labels, pred_test))\n",
    "print(\"Recall: \", recall_score(test_labels, pred_test))\n",
    "print(\"Classification Report: \\n\", classification_report(test_labels, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "for param in resnet50_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class New_ResNet50(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(New_ResNet50, self).__init__()\n",
    "        # Cargamos el modelo pre-entrenado con los parámetros congelados\n",
    "        # Cambiamos la aquitectura de la capa fully-connected para que tenga 1024 neuronas\n",
    "        self.base_model = base_model\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 1024)\n",
    "\n",
    "        # Descongelamos los parámetros de la capa fully-connected para que se actualicen durante el entrenamiento\n",
    "        # ya que hemos cambiado su arquitectura\n",
    "        for param in self.base_model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Añadimos varias capas fully-connected para la clasificación\n",
    "        # https://link.springer.com/article/10.1007/s00521-022-07793-2#Sec14\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.base_model(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tfm/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "resnet50_model = New_ResNet50(resnet50_model).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet50_model.parameters()), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "lr_sch = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n",
    "                                        patience=1, threshold=0.0001, threshold_mode='rel', \n",
    "                                        cooldown=0, min_lr=0, eps=1e-15, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 369/369 [00:11<00:00, 30.81it/s, accuracy=0.764, loss=0.536]\n",
      "Epoch 1: 100%|██████████| 369/369 [00:09<00:00, 38.22it/s, accuracy=0.799, loss=0.506]\n",
      "Epoch 2: 100%|██████████| 369/369 [00:09<00:00, 38.53it/s, accuracy=0.805, loss=0.499]\n",
      "Epoch 3: 100%|██████████| 369/369 [00:09<00:00, 38.72it/s, accuracy=0.809, loss=0.492]\n",
      "Epoch 4: 100%|██████████| 369/369 [00:09<00:00, 38.71it/s, accuracy=0.816, loss=0.487]\n",
      "Epoch 5: 100%|██████████| 369/369 [00:09<00:00, 38.64it/s, accuracy=0.818, loss=0.484]\n",
      "Epoch 6: 100%|██████████| 369/369 [00:09<00:00, 38.74it/s, accuracy=0.83, loss=0.474] \n",
      "Epoch 7: 100%|██████████| 369/369 [00:09<00:00, 38.67it/s, accuracy=0.831, loss=0.469]\n",
      "Epoch 8: 100%|██████████| 369/369 [00:09<00:00, 38.58it/s, accuracy=0.84, loss=0.465] \n",
      "Epoch 9: 100%|██████████| 369/369 [00:09<00:00, 38.71it/s, accuracy=0.846, loss=0.458]\n",
      "Epoch 10: 100%|██████████| 369/369 [00:09<00:00, 38.65it/s, accuracy=0.851, loss=0.454]\n",
      "Epoch 11: 100%|██████████| 369/369 [00:09<00:00, 38.78it/s, accuracy=0.852, loss=0.454]\n",
      "Epoch 12: 100%|██████████| 369/369 [00:09<00:00, 38.68it/s, accuracy=0.855, loss=0.451]\n",
      "Epoch 13: 100%|██████████| 369/369 [00:09<00:00, 38.62it/s, accuracy=0.858, loss=0.449]\n",
      "Epoch 14: 100%|██████████| 369/369 [00:09<00:00, 38.61it/s, accuracy=0.875, loss=0.433]\n",
      "Epoch 15: 100%|██████████| 369/369 [00:09<00:00, 38.59it/s, accuracy=0.874, loss=0.434]\n",
      "Epoch 16: 100%|██████████| 369/369 [00:09<00:00, 38.55it/s, accuracy=0.876, loss=0.432]\n",
      "Epoch 17: 100%|██████████| 369/369 [00:09<00:00, 38.64it/s, accuracy=0.874, loss=0.433]\n",
      "Epoch 18: 100%|██████████| 369/369 [00:09<00:00, 38.54it/s, accuracy=0.878, loss=0.431]\n",
      "Epoch 19: 100%|██████████| 369/369 [00:09<00:00, 38.57it/s, accuracy=0.882, loss=0.425]\n",
      "Epoch 20: 100%|██████████| 369/369 [00:09<00:00, 38.70it/s, accuracy=0.891, loss=0.418]\n",
      "Epoch 21: 100%|██████████| 369/369 [00:09<00:00, 38.24it/s, accuracy=0.888, loss=0.42] \n",
      "Epoch 22: 100%|██████████| 369/369 [00:09<00:00, 37.58it/s, accuracy=0.894, loss=0.416]\n",
      "Epoch 23: 100%|██████████| 369/369 [00:09<00:00, 37.77it/s, accuracy=0.899, loss=0.411]\n",
      "Epoch 24: 100%|██████████| 369/369 [00:09<00:00, 37.98it/s, accuracy=0.895, loss=0.413]\n",
      "Epoch 25: 100%|██████████| 369/369 [00:09<00:00, 38.04it/s, accuracy=0.903, loss=0.408]\n",
      "Epoch 26: 100%|██████████| 369/369 [00:09<00:00, 38.04it/s, accuracy=0.902, loss=0.408]\n",
      "Epoch 27: 100%|██████████| 369/369 [00:09<00:00, 38.12it/s, accuracy=0.906, loss=0.404]\n",
      "Epoch 28: 100%|██████████| 369/369 [00:09<00:00, 38.08it/s, accuracy=0.905, loss=0.405]\n",
      "Epoch 29: 100%|██████████| 369/369 [00:09<00:00, 37.56it/s, accuracy=0.906, loss=0.404]\n",
      "Epoch 30: 100%|██████████| 369/369 [00:09<00:00, 37.88it/s, accuracy=0.911, loss=0.399]\n",
      "Epoch 31: 100%|██████████| 369/369 [00:09<00:00, 37.86it/s, accuracy=0.917, loss=0.393]\n",
      "Epoch 32: 100%|██████████| 369/369 [00:09<00:00, 37.88it/s, accuracy=0.923, loss=0.388]\n",
      "Epoch 33: 100%|██████████| 369/369 [00:09<00:00, 37.88it/s, accuracy=0.923, loss=0.387]\n",
      "Epoch 34: 100%|██████████| 369/369 [00:09<00:00, 37.88it/s, accuracy=0.923, loss=0.387]\n",
      "Epoch 35: 100%|██████████| 369/369 [00:09<00:00, 37.93it/s, accuracy=0.927, loss=0.384]\n",
      "Epoch 36: 100%|██████████| 369/369 [00:09<00:00, 37.96it/s, accuracy=0.93, loss=0.381] \n",
      "Epoch 37: 100%|██████████| 369/369 [00:09<00:00, 37.77it/s, accuracy=0.928, loss=0.382]\n",
      "Epoch 38: 100%|██████████| 369/369 [00:09<00:00, 37.87it/s, accuracy=0.934, loss=0.377]\n",
      "Epoch 39: 100%|██████████| 369/369 [00:09<00:00, 37.93it/s, accuracy=0.93, loss=0.38]  \n",
      "Epoch 40: 100%|██████████| 369/369 [00:09<00:00, 37.91it/s, accuracy=0.93, loss=0.38]  \n",
      "Epoch 41: 100%|██████████| 369/369 [00:09<00:00, 37.87it/s, accuracy=0.936, loss=0.376]\n",
      "Epoch 42: 100%|██████████| 369/369 [00:09<00:00, 37.94it/s, accuracy=0.934, loss=0.376]\n",
      "Epoch 43: 100%|██████████| 369/369 [00:09<00:00, 37.86it/s, accuracy=0.94, loss=0.371] \n",
      "Epoch 44: 100%|██████████| 369/369 [00:09<00:00, 37.95it/s, accuracy=0.939, loss=0.372]\n",
      "Epoch 45: 100%|██████████| 369/369 [00:09<00:00, 37.94it/s, accuracy=0.94, loss=0.371] \n",
      "Epoch 46: 100%|██████████| 369/369 [00:09<00:00, 37.95it/s, accuracy=0.942, loss=0.37] \n",
      "Epoch 47: 100%|██████████| 369/369 [00:09<00:00, 37.87it/s, accuracy=0.943, loss=0.369]\n",
      "Epoch 48: 100%|██████████| 369/369 [00:09<00:00, 37.91it/s, accuracy=0.944, loss=0.366]\n",
      "Epoch 49: 100%|██████████| 369/369 [00:09<00:00, 37.94it/s, accuracy=0.944, loss=0.369]\n",
      "Epoch 50: 100%|██████████| 369/369 [00:09<00:00, 37.82it/s, accuracy=0.943, loss=0.368]\n",
      "Epoch 51: 100%|██████████| 369/369 [00:09<00:00, 37.89it/s, accuracy=0.945, loss=0.367]\n",
      "Epoch 52: 100%|██████████| 369/369 [00:09<00:00, 37.86it/s, accuracy=0.947, loss=0.365]\n",
      "Epoch 53: 100%|██████████| 369/369 [00:09<00:00, 37.94it/s, accuracy=0.944, loss=0.368]\n",
      "Epoch 54: 100%|██████████| 369/369 [00:09<00:00, 37.96it/s, accuracy=0.947, loss=0.366]\n",
      "Epoch 55: 100%|██████████| 369/369 [00:09<00:00, 37.83it/s, accuracy=0.947, loss=0.365]\n",
      "Epoch 56: 100%|██████████| 369/369 [00:09<00:00, 37.92it/s, accuracy=0.942, loss=0.369]\n",
      "Epoch 57: 100%|██████████| 369/369 [00:09<00:00, 37.93it/s, accuracy=0.945, loss=0.366]\n",
      "Epoch 58: 100%|██████████| 369/369 [00:09<00:00, 37.85it/s, accuracy=0.948, loss=0.364]\n",
      "Epoch 59: 100%|██████████| 369/369 [00:09<00:00, 37.88it/s, accuracy=0.948, loss=0.363]\n",
      "Epoch 60: 100%|██████████| 369/369 [00:09<00:00, 37.97it/s, accuracy=0.947, loss=0.365]\n",
      "Epoch 61: 100%|██████████| 369/369 [00:09<00:00, 37.75it/s, accuracy=0.943, loss=0.367]\n",
      "Epoch 62: 100%|██████████| 369/369 [00:09<00:00, 37.87it/s, accuracy=0.947, loss=0.365]\n",
      "Epoch 63: 100%|██████████| 369/369 [00:09<00:00, 37.80it/s, accuracy=0.946, loss=0.365]\n",
      "Epoch 64: 100%|██████████| 369/369 [00:09<00:00, 37.92it/s, accuracy=0.952, loss=0.361]\n",
      "Epoch 65: 100%|██████████| 369/369 [00:09<00:00, 37.89it/s, accuracy=0.95, loss=0.361] \n",
      "Epoch 66: 100%|██████████| 369/369 [00:09<00:00, 37.98it/s, accuracy=0.948, loss=0.364]\n",
      "Epoch 67: 100%|██████████| 369/369 [00:09<00:00, 37.93it/s, accuracy=0.949, loss=0.363]\n",
      "Epoch 68: 100%|██████████| 369/369 [00:09<00:00, 37.67it/s, accuracy=0.946, loss=0.366]\n",
      "Epoch 69: 100%|██████████| 369/369 [00:09<00:00, 37.55it/s, accuracy=0.947, loss=0.364]\n",
      "Epoch 70: 100%|██████████| 369/369 [00:09<00:00, 37.66it/s, accuracy=0.951, loss=0.362]\n",
      "Epoch 71: 100%|██████████| 369/369 [00:09<00:00, 37.63it/s, accuracy=0.949, loss=0.363]\n",
      "Epoch 72: 100%|██████████| 369/369 [00:09<00:00, 37.72it/s, accuracy=0.946, loss=0.364]\n",
      "Epoch 73: 100%|██████████| 369/369 [00:09<00:00, 37.71it/s, accuracy=0.948, loss=0.364]\n",
      "Epoch 74: 100%|██████████| 369/369 [00:09<00:00, 37.76it/s, accuracy=0.949, loss=0.364]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    resnet50_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images_batch = batch['image'].to(DEVICE)\n",
    "        labels_batch = batch['label'].to(DEVICE).long()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        output = resnet50_model(images_batch)\n",
    "        loss_value = loss(output, labels_batch)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.item()\n",
    "        epoch_accuracy += calculate_accuracy(output, labels_batch)\n",
    "        progress_bar.set_postfix(loss=epoch_loss / (step + 1), accuracy=epoch_accuracy / (step + 1))\n",
    "    lr_sch.step(epoch_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8238888888888889\n",
      "Confusion Matrix: \n",
      " [[803 170]\n",
      " [147 680]]\n",
      "Precision:  0.8\n",
      "Recall:  0.8222490931076178\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       973\n",
      "           1       0.80      0.82      0.81       827\n",
      "\n",
      "    accuracy                           0.82      1800\n",
      "   macro avg       0.82      0.82      0.82      1800\n",
      "weighted avg       0.82      0.82      0.82      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo en el conjunto de test\n",
    "resnet50_model.eval()\n",
    "pred_test = resnet50_model(test_images.to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, pred_test))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_labels, pred_test))\n",
    "print(\"Precision: \", precision_score(test_labels, pred_test))\n",
    "print(\"Recall: \", recall_score(test_labels, pred_test))\n",
    "print(\"Classification Report: \\n\", classification_report(test_labels, pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
